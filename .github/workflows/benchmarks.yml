name: Benchmarks

on:
  pull_request:
    branches: [ main ]
  push:
    branches: [ main ]

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  benchmark:
    runs-on: macos-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Swift
      uses: swift-actions/setup-swift@v2
      with:
        swift-version: "6.0"

    - name: Run benchmarks
      id: benchmark
      run: |
        echo "Running benchmarks..."
        swift test --filter testPerformance 2>&1 | tee benchmark_output.txt

    - name: Parse and format results
      run: |
        python3 << 'EOF'
        import re
        from datetime import datetime
        import subprocess

        # Get system info
        try:
            cpu_brand = subprocess.check_output(['sysctl', '-n', 'machdep.cpu.brand_string']).decode().strip()
            mem_bytes = int(subprocess.check_output(['sysctl', '-n', 'hw.memsize']).decode().strip())
            mem_gb = mem_bytes / (1024**3)
            system_info = f"{cpu_brand}, {mem_gb:.0f} GB RAM"
        except:
            system_info = "macOS (GitHub Actions)"

        # Read benchmark output
        with open('benchmark_output.txt', 'r') as f:
            content = f.read()

        # Find all performance test results
        pattern = r"testPerformance(\w+).*?average: ([\d.]+)"
        matches = re.findall(pattern, content)

        # Start markdown output
        output = ["# ðŸš€ KeyValueStore Performance Benchmarks\n"]
        output.append("*Optimized with double hashing, memcmp equality, and derived hash2*\n")
        output.append(f"**Test Hardware:** {system_info}\n")

        # Core operations section
        output.append("## Core Operations (100 ops)\n")
        output.append("| Operation | Time | Per-Op | Main Thread |")
        output.append("|-----------|------|--------|-------------|")

        core_ops = {
            "Insert": ("Insert", 100),
            "LookupHit": ("Lookup (hit)", 100),
            "LookupMiss": ("Lookup (miss)", 100),
            "Update": ("Update", 100),
            "Remove": ("Remove", 200),
            "Contains": ("Contains", 100),
        }

        for test_name, avg_time in matches:
            if test_name in core_ops:
                avg_time_f = float(avg_time)
                op_name, ops = core_ops[test_name]

                total_ms = avg_time_f * 1000
                per_op_us = (avg_time_f * 1_000_000) / ops

                if per_op_us < 1:
                    per_op = "<1 Î¼s"
                elif per_op_us < 1000:
                    per_op = f"{per_op_us:.1f} Î¼s"
                else:
                    per_op = f"{per_op_us/1000:.2f} ms"

                if total_ms < 10:
                    status = "âœ… Excellent"
                elif total_ms < 50:
                    status = "âœ… Good"
                elif total_ms < 100:
                    status = "âš ï¸ OK"
                else:
                    status = "âŒ Review"

                output.append(f"| {op_name} | {total_ms:.1f}ms | {per_op} | {status} |")

        # Load factor performance
        output.append("\n## Load Factor Performance (10,000 lookups)\n")
        output.append("| Load % | Time | Degradation | Status |")
        output.append("|--------|------|-------------|--------|")

        load_factors = {
            "LoadFactor25Percent": ("25%", None),
            "LoadFactor50Percent": ("50%", None),
            "LoadFactor75Percent": ("75%", None),
            "LoadFactor90Percent": ("90%", None),
            "LoadFactor99Percent": ("99%", None),
        }

        baseline = None
        for test_name, avg_time in matches:
            if test_name in load_factors:
                avg_time_f = float(avg_time)
                load_name = load_factors[test_name][0]

                if baseline is None:
                    baseline = avg_time_f
                    degradation = "baseline"
                else:
                    ratio = avg_time_f / baseline
                    degradation = f"{ratio:.1f}x"

                total_ms = avg_time_f * 1000

                if avg_time_f < 0.050:
                    status = "âœ… Excellent"
                elif avg_time_f < 0.100:
                    status = "âœ… Good"
                elif avg_time_f < 0.150:
                    status = "âš ï¸ OK"
                else:
                    status = "âŒ Slow"

                output.append(f"| {load_name} | {total_ms:.0f}ms | {degradation} | {status} |")

        # Key length impact
        output.append("\n## Key Length Impact (100 ops)\n")
        output.append("| Key Length | Time | Per-Op |")
        output.append("|------------|------|--------|")

        key_tests = {
            "ShortKeys": "Short (2-3 chars)",
            "MediumKeys": "Medium (~25 chars)",
            "LongKeys": "Long (64 chars)",
        }

        for test_name, avg_time in matches:
            if test_name in key_tests:
                avg_time_f = float(avg_time)
                key_name = key_tests[test_name]

                total_ms = avg_time_f * 1000
                per_op_us = (avg_time_f * 1_000_000) / 100

                output.append(f"| {key_name} | {total_ms:.1f}ms | {per_op_us:.1f} Î¼s |")

        # Main thread budget
        output.append("\n## Main Thread Guidelines")
        output.append("- âœ… **Excellent**: <10ms - Perfect for UI interactions")
        output.append("- âœ… **Good**: 10-50ms - Acceptable for most operations")
        output.append("- âš ï¸ **OK**: 50-100ms - Use with caution on main thread")
        output.append("- âŒ **Review**: >100ms - Consider background thread")
        output.append("\n*Target: 16.67ms per frame @ 60fps, 8.33ms @ 120fps*")

        # Summary
        total_tests = len(re.findall(r"Test Case.*passed", content))
        output.append(f"\n---\n**Total tests:** {total_tests} passed | _Generated {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_")

        # Write to file
        with open('benchmark_results.md', 'w') as f:
            f.write('\n'.join(output))

        print('\n'.join(output))
        EOF

    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: thollander/actions-comment-pull-request@v2
      with:
        filePath: benchmark_results.md
        comment_tag: benchmark-results

    - name: Comment commit with results
      if: github.event_name == 'push'
      uses: peter-evans/commit-comment@v3
      with:
        body-path: benchmark_results.md

name: Benchmarks

on:
  pull_request:
    branches: [ main ]
  push:
    branches: [ main ]

permissions:
  contents: read
  pull-requests: write
  issues: write

jobs:
  benchmark:
    runs-on: macos-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Set up Swift
      uses: swift-actions/setup-swift@v2
      with:
        swift-version: "6.0"

    - name: Run benchmarks
      id: benchmark
      run: |
        echo "Running benchmarks..."
        swift test 2>&1 | tee benchmark_output.txt

    - name: Parse and format results
      run: |
        python3 << 'EOF'
        import re
        from datetime import datetime

        # Read benchmark output
        with open('benchmark_output.txt', 'r') as f:
            content = f.read()

        # Find all performance test results
        pattern = r"testPerformance(\w+).*?average: ([\d.]+).*?relative standard deviation: ([\d.]+)%"
        matches = re.findall(pattern, content)

        # Start markdown output
        output = ["## Performance Analysis\n"]
        output.append("| Operation | Total Time | Operations | **Per-Op Time** | Main Thread Safe? |")
        output.append("|-----------|-----------|-----------|-----------------|-------------------|")

        # Define operation counts and special handling
        op_counts = {
            "Insert": 100,
            "LookupHit": 100,
            "LookupMiss": 100,
            "Update": 100,
            "Remove": 200,  # 100 inserts + 100 removes
            "Count": 100,
            "Keys": 1,
            "ToDictionary": 1,
            "MixedOperations": 136  # 50+50+25+10+1+1
        }

        op_names = {
            "Insert": "Insert",
            "LookupHit": "Lookup (hit)",
            "LookupMiss": "Lookup (miss)",
            "Update": "Update",
            "Remove": "Remove",
            "Count": "Count",
            "Keys": "Keys (100 items)",
            "ToDictionary": "toDictionary (100)",
            "MixedOperations": "Mixed Operations"
        }

        for test_name, avg_time, std_dev in matches:
            avg_time_f = float(avg_time)
            total_time = f"{avg_time_f * 1000:.1f}ms"

            ops = op_counts.get(test_name, 1)
            op_name = op_names.get(test_name, test_name)

            # Calculate per-operation time
            per_op_us = (avg_time_f * 1_000_000) / ops

            if per_op_us < 1:
                per_op = f"**<1 μs**"
            elif per_op_us < 1000:
                per_op = f"**{per_op_us:.0f} μs**"
            else:
                per_op = f"**{per_op_us/1000:.1f} ms**"

            # Main thread safety (16.67ms budget for 60fps)
            if per_op_us < 100:
                main_thread = "✅ Excellent"
            elif per_op_us < 1000:
                main_thread = "✅ Good"
            elif per_op_us < 5000:
                main_thread = "⚠️ OK"
            else:
                main_thread = "❌ Slow"

            output.append(f"| {op_name} | {total_time} | {ops} | {per_op} | {main_thread} |")

        # Add main thread budget info
        output.append("")
        output.append("### Main Thread Budget")
        output.append("For smooth 60fps: **16.67ms per frame**")
        output.append("For ProMotion 120fps: **8.33ms per frame**")
        output.append("")

        # Add footer
        total_tests = len(re.findall(r"Test Case.*passed", content))
        output.append(f"**Total tests:** {total_tests} passed")
        output.append("")
        output.append(f"_Generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_")

        # Write to file
        with open('benchmark_results.md', 'w') as f:
            f.write('\n'.join(output))

        print('\n'.join(output))
        EOF

    - name: Comment PR with results
      if: github.event_name == 'pull_request'
      uses: thollander/actions-comment-pull-request@v2
      with:
        filePath: benchmark_results.md
        comment_tag: benchmark-results

    - name: Comment commit with results
      if: github.event_name == 'push'
      uses: peter-evans/commit-comment@v3
      with:
        body-path: benchmark_results.md
